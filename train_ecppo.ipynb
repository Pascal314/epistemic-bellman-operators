{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "import numpy as np\n",
    "import optax\n",
    "from flax.linen.initializers import constant, orthogonal, variance_scaling, normal\n",
    "from typing import Sequence, NamedTuple, Any\n",
    "from functools import partial\n",
    "from flax.training.train_state import TrainState\n",
    "import distrax\n",
    "import gymnax\n",
    "from gymnax.wrappers.purerl import LogWrapper, FlattenObservationWrapper\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import tqdm\n",
    "import pickle\n",
    "\n",
    "def subsample_with_end(x, subsampling, axis=0):\n",
    "    leftover = x.shape[axis] % subsampling\n",
    "    return x[leftover-1::subsampling]\n",
    "\n",
    "def process_out(out, subsampling=1):\n",
    "    # metrics: (-1: num_envs, -2: num_steps) (check ? not sure.)\n",
    "    metrics = jax.tree_util.tree_map(lambda x: x.mean(axis=(-1, -2)), out['metrics'])\n",
    "    loss_info = jax.tree_util.tree_map(lambda x: x.mean(axis=(-1)), out['loss_info'])\n",
    "\n",
    "    output = dict(\n",
    "        metrics=metrics, loss_info=loss_info\n",
    "    )\n",
    "\n",
    "    output = jax.tree_util.tree_map(lambda x: x.reshape(x.shape[0], x.shape[1] // subsampling, subsampling, *x.shape[2:]).mean(axis=2), output)\n",
    "    output[\"runner_state\"] = out[\"runner_state\"]\n",
    "\n",
    "    return output\n",
    "\n",
    "def tree_size(tree):\n",
    "    return sum(jax.tree_util.tree_leaves(jax.tree_util.tree_map(lambda x: x.nbytes, tree)))\n",
    "\n",
    "def ss_plot(out, ax=None, quantiles=True, individual=False, color='blue', subsampled=1, label=None, sem=False, smoothing=None, skip_first=False, **kwargs):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(1, 1)\n",
    "\n",
    "    n_runs = out[\"metrics\"][\"returned_episode_returns\"].shape[0]\n",
    "\n",
    "    episode_returns = out[\"metrics\"][\"returned_episode_returns\"]\n",
    "\n",
    "    mean = episode_returns.mean(axis=0)\n",
    "\n",
    "    sorted = jnp.sort(episode_returns, axis=0)\n",
    "\n",
    "    std = episode_returns.std(axis=0)\n",
    "\n",
    "    if not (smoothing is None):\n",
    "        mean = smoothing(mean)\n",
    "        std = smoothing(std)\n",
    "    x_axis = range(0, mean.shape[0] * subsampled, subsampled)\n",
    "    skip_first = int(skip_first)\n",
    "\n",
    "    ax.plot(x_axis[skip_first:], mean[skip_first:], color=color, label=label, **kwargs)\n",
    "    if quantiles:\n",
    "        ax.fill_between(x_axis[skip_first:], sorted[1, skip_first:], sorted[-2, skip_first:], alpha=0.2, color=color)\n",
    "    else:\n",
    "        factor = 1 / np.sqrt(n_runs) if sem else 1\n",
    "        ax.fill_between(x_axis[skip_first:], mean[skip_first:] - factor * std[skip_first:], mean[skip_first:] + factor * std[skip_first:], alpha=0.2, color=color)\n",
    "\n",
    "    if individual:\n",
    "        ax.plot(episode_returns.reshape(n_runs, -1).T, linestyle=\":\", color=color)\n",
    "\n",
    "def save_results(path, results, configs, force=False):\n",
    "    if not isinstance(path, Path):\n",
    "        path = Path(path)\n",
    "    if not force:\n",
    "        if Path(path).is_file():\n",
    "            path += '_0'\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump((results, configs), f)\n",
    "\n",
    "def drop_runner_state(results):\n",
    "    out = {}\n",
    "    for name, value in results.items():\n",
    "        out[name] = dict(loss_info=value['loss_info'], metrics=value['metrics'])\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble-ECPPO + Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "import numpy as np\n",
    "import optax\n",
    "from flax.linen.initializers import constant, orthogonal, variance_scaling, normal\n",
    "from typing import Sequence, NamedTuple, Any\n",
    "from functools import partial\n",
    "from flax.training.train_state import TrainState\n",
    "import distrax\n",
    "import gymnax\n",
    "from gymnax.wrappers.purerl import LogWrapper, FlattenObservationWrapper\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import pickle\n",
    "# warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "# warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "\n",
    "\n",
    "BatchDense = nn.vmap(\n",
    "    nn.Dense,\n",
    "    in_axes=-1, out_axes=-1,\n",
    "    variable_axes={'params': 0,},\n",
    "    split_rngs={'params': True},\n",
    ")\n",
    "\n",
    "prior_bias_init = normal(0.1)\n",
    "\n",
    "class ActorEpistemicCritic(nn.Module):\n",
    "    action_dim: Sequence[int]\n",
    "    ensemble_size: int\n",
    "    activation: str = \"tanh\"\n",
    "    beta: float = 0.\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        if self.activation == \"relu\":\n",
    "            activation = nn.relu\n",
    "        else:\n",
    "            activation = nn.tanh\n",
    "        actor_mean = nn.Dense(\n",
    "            64, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0)\n",
    "        )(x)\n",
    "        actor_mean = activation(actor_mean)\n",
    "        actor_mean = nn.Dense(\n",
    "            64, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0)\n",
    "        )(actor_mean)\n",
    "        actor_mean = activation(actor_mean)\n",
    "        actor_mean = nn.Dense(\n",
    "            self.action_dim, kernel_init=orthogonal(0.01), bias_init=constant(0.0)\n",
    "        )(actor_mean)\n",
    "        pi = distrax.Categorical(logits=actor_mean)\n",
    "        \n",
    "        batched_x = x.reshape(*x.shape, 1).repeat(self.ensemble_size, axis=-1)\n",
    "\n",
    "        critic = BatchDense(\n",
    "            64, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0)\n",
    "        )(batched_x)\n",
    "        critic = activation(critic)\n",
    "        critic = BatchDense(\n",
    "            64, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0)\n",
    "        )(critic)\n",
    "        critic = activation(critic)\n",
    "        critic = BatchDense(1, kernel_init=orthogonal(1.0), bias_init=constant(0.0))(\n",
    "            critic\n",
    "        )\n",
    "\n",
    "        prior_critic = BatchDense(\n",
    "            64, kernel_init=orthogonal(np.sqrt(2)), bias_init=prior_bias_init\n",
    "        )(batched_x)\n",
    "        prior_critic = activation(prior_critic)\n",
    "        prior_critic = BatchDense(\n",
    "            64, kernel_init=orthogonal(np.sqrt(2)), bias_init=prior_bias_init\n",
    "        )(prior_critic)\n",
    "        prior_critic = activation(prior_critic)\n",
    "        prior_critic = BatchDense(1, kernel_init=orthogonal(1.0), bias_init=prior_bias_init)(\n",
    "            prior_critic\n",
    "        )\n",
    "        return pi, jnp.squeeze(critic, axis=-2) + self.beta * jnp.squeeze(jax.lax.stop_gradient(prior_critic), axis=-2)\n",
    "    \n",
    "\n",
    "\n",
    "class Transition(NamedTuple):\n",
    "    done: jnp.ndarray\n",
    "    action: jnp.ndarray\n",
    "    value: jnp.ndarray\n",
    "    reward: jnp.ndarray\n",
    "    log_prob: jnp.ndarray\n",
    "    obs: jnp.ndarray\n",
    "    info: jnp.ndarray\n",
    "\n",
    "def make_train(config):\n",
    "    config[\"NUM_UPDATES\"] = (\n",
    "        config[\"TOTAL_TIMESTEPS\"] // config[\"NUM_STEPS\"] // config[\"NUM_ENVS\"]\n",
    "    )\n",
    "    config[\"MINIBATCH_SIZE\"] = (\n",
    "        config[\"NUM_ENVS\"] * config[\"NUM_STEPS\"] // config[\"NUM_MINIBATCHES\"]\n",
    "    )\n",
    "\n",
    "    env_kwargs = {}\n",
    "    if config[\"ENV_NAME\"] == \"DeepSea-bsuite\":\n",
    "        env_kwargs[\"size\"] = 15\n",
    "    env, env_params = gymnax.make(config[\"ENV_NAME\"], **env_kwargs)\n",
    "    env = FlattenObservationWrapper(env)\n",
    "    env = LogWrapper(env)\n",
    "\n",
    "    def linear_schedule(count):\n",
    "        frac = 1.0 - (count // (config[\"NUM_MINIBATCHES\"] * config[\"UPDATE_EPOCHS\"])) / config[\"NUM_UPDATES\"]\n",
    "        return config[\"LR\"] * frac\n",
    "\n",
    "    def train(rng):\n",
    "\n",
    "        # INIT NETWORK\n",
    "        network = ActorEpistemicCritic(env.action_space(env_params).n, activation=config[\"ACTIVATION\"], ensemble_size=config[\"ENSEMBLE_SIZE\"], beta=config[\"BETA\"])\n",
    "        rng, _rng = jax.random.split(rng)\n",
    "        init_x = jnp.zeros(env.observation_space(env_params).shape)\n",
    "        network_params = network.init(_rng, init_x)\n",
    "\n",
    "        # print(jax.tree_util.tree_map(lambda x: x.shape, network_params))\n",
    "\n",
    "        if config[\"ANNEAL_LR\"]:\n",
    "            tx = optax.chain(\n",
    "                optax.clip_by_global_norm(config[\"MAX_GRAD_NORM\"]),\n",
    "                optax.adam(learning_rate=linear_schedule, eps=1e-5),\n",
    "            )\n",
    "        else:\n",
    "            tx = optax.chain(optax.clip_by_global_norm(config[\"MAX_GRAD_NORM\"]), optax.adam(config[\"LR\"], eps=1e-5))\n",
    "        train_state = TrainState.create(\n",
    "            apply_fn=network.apply,\n",
    "            params=network_params,\n",
    "            tx=tx,\n",
    "        )\n",
    "\n",
    "        if \"UNCERTAINTY_SHIFT\" not in config.keys():\n",
    "            config[\"UNCERTAINTY_SHIFT\"] = 0.3\n",
    "        \n",
    "        if \"UNCERTAINTY_MULTIPLIER\" not in config.keys():\n",
    "            config[\"UNCERTAINTY_MULTIPLIER\"] = 15.\n",
    "\n",
    "        if \"UNCERTAINTY_BASE\" not in config.keys():\n",
    "            config[\"UNCERTAINTY_BASE\"] = 0.5\n",
    "\n",
    "        if \"UNCERTAINTY_SCALE\" not in config.keys():\n",
    "            config[\"UNCERTAINTY_SCALE\"] = 1.5\n",
    "\n",
    "        # INIT ENV\n",
    "        rng, _rng = jax.random.split(rng)\n",
    "        reset_rng = jax.random.split(_rng, config[\"NUM_ENVS\"])\n",
    "        obsv, env_state = jax.vmap(env.reset, in_axes=(0, None))(reset_rng,  env_params)\n",
    "\n",
    "        # TRAIN LOOP\n",
    "        def _update_step(runner_state, unused):\n",
    "            # COLLECT TRAJECTORIES\n",
    "            def _env_step(runner_state, unused):\n",
    "                train_state, env_state, last_obs, rng = runner_state\n",
    "\n",
    "                # SELECT ACTION\n",
    "                rng, _rng = jax.random.split(rng)\n",
    "                pi, value = network.apply(train_state.params, last_obs)\n",
    "                action = pi.sample(seed=_rng)\n",
    "                log_prob = pi.log_prob(action)\n",
    "\n",
    "                # STEP ENV\n",
    "                rng, _rng = jax.random.split(rng)\n",
    "                rng_step = jax.random.split(_rng, config[\"NUM_ENVS\"])\n",
    "                obsv, env_state, reward, done, info = jax.vmap(env.step, in_axes=(0,0,0,None))(\n",
    "                    rng_step, env_state, action, env_params\n",
    "                )\n",
    "                transition = Transition(\n",
    "                    done, action, value, reward, log_prob, last_obs, info\n",
    "                )\n",
    "                runner_state = (train_state, env_state, obsv, rng)\n",
    "                return runner_state, transition\n",
    "\n",
    "            runner_state, traj_batch = jax.lax.scan(\n",
    "                _env_step, runner_state, None, config[\"NUM_STEPS\"]\n",
    "            )\n",
    "\n",
    "            # CALCULATE ADVANTAGE\n",
    "            train_state, env_state, last_obs, rng = runner_state\n",
    "            _, last_val = network.apply(train_state.params, last_obs)\n",
    "            def _calculate_gae(traj_batch, last_val):\n",
    "                def _get_advantages(gae_and_next_value, transition):\n",
    "                    gae, next_value = gae_and_next_value\n",
    "                    done, value, reward = (\n",
    "                        transition.done,\n",
    "                        transition.value,\n",
    "                        transition.reward,\n",
    "                    )\n",
    "                    delta = reward[..., None] + config[\"GAMMA\"] * next_value * (1 - done[..., None]) - value\n",
    "                    gae = (\n",
    "                        delta\n",
    "                        + config[\"GAMMA\"] * config[\"GAE_LAMBDA\"] * (1 - done[..., None]) * gae\n",
    "                    )\n",
    "                    return (gae, value), gae\n",
    "\n",
    "                _, advantages = jax.lax.scan(\n",
    "                    _get_advantages,\n",
    "                    (jnp.zeros_like(last_val), last_val),\n",
    "                    traj_batch,\n",
    "                    reverse=True,\n",
    "                    unroll=16,\n",
    "                )\n",
    "                return advantages, advantages + traj_batch.value\n",
    "\n",
    "            advantages, targets = _calculate_gae(traj_batch, last_val)\n",
    "\n",
    "            # UPDATE NETWORK\n",
    "            def _update_epoch(update_state, unused):\n",
    "                def _update_minbatch(train_state, batch_info):\n",
    "                    traj_batch, advantages, targets = batch_info\n",
    "\n",
    "                    def _loss_fn(params, traj_batch, gae, targets):\n",
    "                        # RERUN NETWORK\n",
    "                        pi, value = network.apply(params, traj_batch.obs)\n",
    "                        log_prob = pi.log_prob(traj_batch.action)\n",
    "\n",
    "                        # CALCULATE VALUE LOSS\n",
    "                        value_pred_clipped = traj_batch.value + (\n",
    "                            value - traj_batch.value\n",
    "                        ).clip(-config[\"CLIP_EPS\"], config[\"CLIP_EPS\"])\n",
    "                        value_losses = jnp.square(value - targets)\n",
    "                        value_losses_clipped = jnp.square(value_pred_clipped - targets)\n",
    "                        value_loss = (\n",
    "                            0.5 * jnp.maximum(value_losses, value_losses_clipped).mean()\n",
    "                        )\n",
    "\n",
    "                        # CALCULATE ACTOR LOSS\n",
    "                        ratio = jnp.exp(log_prob - traj_batch.log_prob)\n",
    "\n",
    "                        gae = (gae - gae.mean()) / (gae.std() + 1e-8)\n",
    "\n",
    "                        gae_uncertainty = gae.std(axis=-1)\n",
    "                        \n",
    "                        if config[\"UNCERTAINTY\"]:\n",
    "                            squashed_uncertainty = config[\"UNCERTAINTY_BASE\"] + config[\"UNCERTAINTY_SCALE\"] * jax.nn.sigmoid( config[\"UNCERTAINTY_MULTIPLIER\"] * (-gae_uncertainty + config[\"UNCERTAINTY_SHIFT\"] ))\n",
    "                        else:\n",
    "                            squashed_uncertainty = jnp.array(1.)\n",
    "\n",
    "                        loss_actor1 = ratio * jnp.mean(gae, axis=-1) # average loss over all values\n",
    "                        loss_actor2 = (\n",
    "                            jnp.clip(\n",
    "                                ratio,\n",
    "                                1.0 - config[\"CLIP_EPS\"] * squashed_uncertainty,\n",
    "                                1.0 + config[\"CLIP_EPS\"] * squashed_uncertainty,\n",
    "                            )\n",
    "                            * jnp.mean(gae, axis=-1)\n",
    "                        )\n",
    "\n",
    "                        percentage_clipped = (jnp.abs(ratio - 1) > config[\"CLIP_EPS\"] * squashed_uncertainty).mean()\n",
    "\n",
    "                        loss_actor = -jnp.minimum(loss_actor1, loss_actor2)\n",
    "\n",
    "                        loss_argmin = (loss_actor1 > loss_actor2).mean()\n",
    "                        loss_actor = loss_actor.mean()\n",
    "                        entropy = pi.entropy().mean()\n",
    "\n",
    "                        total_loss = (\n",
    "                            loss_actor\n",
    "                            + config[\"VF_COEF\"] * value_loss\n",
    "                            - config[\"ENT_COEF\"] * entropy\n",
    "                        )\n",
    "                        return total_loss, dict(value_loss=value_loss, \n",
    "                                                loss_actor=loss_actor, \n",
    "                                                entropy=entropy, \n",
    "                                                mean_uncertainty=gae_uncertainty.mean(), \n",
    "                                                min_uncertainty=gae_uncertainty.min(), \n",
    "                                                max_uncertainty=gae_uncertainty.max(),\n",
    "                                                min_gae = gae.min(),\n",
    "                                                max_gae = gae.max(),\n",
    "                                                percentage_clipped=percentage_clipped,\n",
    "                                                loss_argmin=loss_argmin,\n",
    "                                                mean_squashed_uncertainty=squashed_uncertainty.mean(),\n",
    "                                                max_squashed_uncertainty=squashed_uncertainty.max(),\n",
    "                                                min_squashed_uncertainty=squashed_uncertainty.min())\n",
    "\n",
    "                    grad_fn = jax.value_and_grad(_loss_fn, has_aux=True)\n",
    "                    total_loss, grads = grad_fn(\n",
    "                        train_state.params, traj_batch, advantages, targets\n",
    "                    )\n",
    "                    train_state = train_state.apply_gradients(grads=grads)\n",
    "                    return train_state, total_loss\n",
    "\n",
    "                train_state, traj_batch, advantages, targets, rng = update_state\n",
    "                rng, _rng = jax.random.split(rng)\n",
    "                batch_size = config[\"MINIBATCH_SIZE\"] * config[\"NUM_MINIBATCHES\"]\n",
    "                assert (\n",
    "                    batch_size == config[\"NUM_STEPS\"] * config[\"NUM_ENVS\"]\n",
    "                ), \"batch size must be equal to number of steps * number of envs\"\n",
    "                permutation = jax.random.permutation(_rng, batch_size)\n",
    "                batch = (traj_batch, advantages, targets)\n",
    "\n",
    "                batch = jax.tree_util.tree_map(\n",
    "                    lambda x: x.reshape((batch_size,) + x.shape[2:]), batch\n",
    "                )\n",
    "                shuffled_batch = jax.tree_util.tree_map(\n",
    "                    lambda x: jnp.take(x, permutation, axis=0), batch\n",
    "                )\n",
    "                minibatches = jax.tree_util.tree_map(\n",
    "                    lambda x: jnp.reshape(\n",
    "                        x, [config[\"NUM_MINIBATCHES\"], -1] + list(x.shape[1:])\n",
    "                    ),\n",
    "                    shuffled_batch,\n",
    "                )\n",
    "                train_state, total_loss = jax.lax.scan(\n",
    "                    _update_minbatch, train_state, minibatches\n",
    "                )\n",
    "                update_state = (train_state, traj_batch, advantages, targets, rng)\n",
    "                return update_state, total_loss\n",
    "\n",
    "            update_state = (train_state, traj_batch, advantages, targets, rng)\n",
    "            update_state, loss_info = jax.lax.scan(\n",
    "                _update_epoch, update_state, None, config[\"UPDATE_EPOCHS\"]\n",
    "            )\n",
    "            train_state = update_state[0]\n",
    "            metric = traj_batch.info\n",
    "            rng = update_state[-1]\n",
    "\n",
    "            runner_state = (train_state, env_state, last_obs, rng)\n",
    "            info = dict(\n",
    "                metrics=metric,\n",
    "                loss_info=loss_info\n",
    "            )\n",
    "            return runner_state, info\n",
    "        \n",
    "        if config[\"SAVE_STATE_FREQ\"]:\n",
    "            outer_updates = config[\"NUM_UPDATES\"] // config[\"SAVE_STATE_FREQ\"]\n",
    "            inner_updates = config[\"SAVE_STATE_FREQ\"]\n",
    "            left_over = config[\"NUM_UPDATES\"] - inner_updates * outer_updates \n",
    "            print(inner_updates, outer_updates, left_over)\n",
    "        else:\n",
    "            inner_updates = config[\"NUM_UPDATES\"]\n",
    "            outer_updates = 1\n",
    "            left_over = 0\n",
    "\n",
    "        # start with the left_over steps:\n",
    "        rng, _rng = jax.random.split(rng)\n",
    "        runner_state = (train_state, env_state, obsv, _rng)\n",
    "        runner_state, initial_info = jax.lax.scan(\n",
    "            _update_step, runner_state, None, left_over\n",
    "        )\n",
    "\n",
    "        def _outer_update_step(update_state, unused):\n",
    "            runner_state, info = jax.lax.scan(\n",
    "                _update_step, update_state, None, inner_updates\n",
    "            )\n",
    "            return runner_state, (runner_state, info)\n",
    "\n",
    "        # then do the remaining updates\n",
    "        runner_state, (states, info) = jax.lax.scan(\n",
    "            _outer_update_step, runner_state, None, outer_updates\n",
    "        )\n",
    "\n",
    "        info = jax.tree_util.tree_map(lambda x: x.reshape(-1, *x.shape[2:]), info)\n",
    "        info = jax.tree_util.tree_map(lambda x, y: jnp.concatenate([x, y], axis=0), initial_info, info)\n",
    "\n",
    "        info[\"runner_state\"] = states\n",
    "              \n",
    "        return info\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "baseline_config = {\n",
    "    \"LR\": 5e-3,\n",
    "    \"NUM_ENVS\": 64,\n",
    "    \"NUM_STEPS\": 128,\n",
    "    \"TOTAL_TIMESTEPS\": 1e7,\n",
    "    \"UPDATE_EPOCHS\": 4,\n",
    "    \"NUM_MINIBATCHES\": 8,\n",
    "    \"GAMMA\": 0.99,\n",
    "    \"GAE_LAMBDA\": 0.95,\n",
    "    \"CLIP_EPS\": 0.2,\n",
    "    \"ENT_COEF\": 0.01,\n",
    "    \"VF_COEF\": 0.5,\n",
    "    \"MAX_GRAD_NORM\": 0.5,\n",
    "    \"ACTIVATION\": \"relu\",\n",
    "    \"ANNEAL_LR\": True,\n",
    "    \"ENSEMBLE_SIZE\": 1,\n",
    "    \"UNCERTAINTY\": False,\n",
    "    \"BETA\": 0.,\n",
    "    \"SAVE_STATE_FREQ\": False,\n",
    "}\n",
    "\n",
    "ecppo_config = {\n",
    "    \"LR\": 5e-3,\n",
    "    \"NUM_ENVS\": 64,\n",
    "    \"NUM_STEPS\": 128,\n",
    "    \"TOTAL_TIMESTEPS\": 1e7,\n",
    "    \"UPDATE_EPOCHS\": 4,\n",
    "    \"NUM_MINIBATCHES\": 8,\n",
    "    \"GAMMA\": 0.99,\n",
    "    \"GAE_LAMBDA\": 0.95,\n",
    "    \"CLIP_EPS\": 0.2,\n",
    "    \"ENT_COEF\": 0.01,\n",
    "    \"VF_COEF\": .5,\n",
    "    \"MAX_GRAD_NORM\": 0.5,\n",
    "    \"ACTIVATION\": \"relu\",\n",
    "    \"ANNEAL_LR\": True,\n",
    "    \"ENSEMBLE_SIZE\": 5,\n",
    "    \"UNCERTAINTY\": True,\n",
    "    \"BETA\": 1.,\n",
    "    \"SAVE_STATE_FREQ\": False,\n",
    "    \"UNCERTAINTY_BASE\": 0.5,\n",
    "    \"UNCERTAINTY_SCALE\": 1.5,\n",
    "    \"UNCERTAINTY_SHIFT\": 0.3,\n",
    "    \"UNCERTAINTY_MULTIPLIER\": 15,\n",
    "}\n",
    "\n",
    "baseline_high_eps = {key: value for key, value in baseline_config.items()}\n",
    "baseline_high_eps[\"CLIP_EPS\"] = 0.4\n",
    "\n",
    "\n",
    "baseline_low_eps = {key: value for key, value in baseline_config.items()}\n",
    "baseline_low_eps[\"CLIP_EPS\"] = 0.1\n",
    "\n",
    "\n",
    "configs = {\n",
    "    'baseline': baseline_config,\n",
    "    'baseline_high_eps': baseline_high_eps,\n",
    "    'baseline_low_eps': baseline_low_eps,\n",
    "    'ens-ecppo': ecppo_config,\n",
    "}\n",
    "\n",
    "load = False\n",
    "\n",
    "if load:\n",
    "    with open('results.pkl', 'rb') as f:\n",
    "        results, _ = pickle.load(f)\n",
    "else:\n",
    "    results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CartPole-v1', 'Acrobot-v1', 'MountainCar-v0', 'Asterix-MinAtar', 'Breakout-MinAtar', 'Freeway-MinAtar', 'SpaceInvaders-MinAtar', 'Catch-bsuite', 'DeepSea-bsuite', 'MemoryChain-bsuite', 'UmbrellaChain-bsuite', 'DiscountingChain-bsuite', 'MNISTBandit-bsuite', 'SimpleBandit-bsuite', 'FourRooms-misc', 'MetaMaze-misc', 'BernoulliBandit-misc', 'GaussianBandit-misc', 'Pong-misc']\n"
     ]
    }
   ],
   "source": [
    "envs = [env for env in gymnax.registered_envs if isinstance(gymnax.make(env)[0].action_space(), gymnax.environments.spaces.Discrete)]\n",
    "\n",
    "print(envs)\n",
    "\n",
    "base = 1e7 // (64 * 128)\n",
    "short_axis = {\n",
    "    \"CartPole-v1\": 200,\n",
    "    \"Pong-misc\": 400,\n",
    "    \"UmbrellaChain-bsuite\": 50,\n",
    "    \"FourRooms-misc\": 100,\n",
    "    \"DiscountingChain-bsuite\": 200,\n",
    "    \"GaussianBandit-misc\": 50,\n",
    "    \"Catch-bsuite\": 50,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 3/19 [04:27<25:52, 97.01s/it, baseline/Asterix-MinAtar]        /home/pascal/.local/share/virtualenvs/epistemically-uncertain-targets-H8pIDgDw/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int32 to dtype=bool with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.\n",
      "  warnings.warn(\n",
      "/home/pascal/.local/share/virtualenvs/epistemically-uncertain-targets-H8pIDgDw/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int32 to dtype=bool with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.\n",
      "  warnings.warn(\n",
      " 16%|█▌        | 3/19 [05:49<25:52, 97.01s/it, baseline_high_eps/Asterix-MinAtar]/home/pascal/.local/share/virtualenvs/epistemically-uncertain-targets-H8pIDgDw/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int32 to dtype=bool with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.\n",
      "  warnings.warn(\n",
      "/home/pascal/.local/share/virtualenvs/epistemically-uncertain-targets-H8pIDgDw/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int32 to dtype=bool with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.\n",
      "  warnings.warn(\n",
      " 16%|█▌        | 3/19 [07:11<25:52, 97.01s/it, baseline_low_eps/Asterix-MinAtar] /home/pascal/.local/share/virtualenvs/epistemically-uncertain-targets-H8pIDgDw/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int32 to dtype=bool with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.\n",
      "  warnings.warn(\n",
      "/home/pascal/.local/share/virtualenvs/epistemically-uncertain-targets-H8pIDgDw/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int32 to dtype=bool with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.\n",
      "  warnings.warn(\n",
      " 16%|█▌        | 3/19 [08:33<25:52, 97.01s/it, ens-ecppo/Asterix-MinAtar]       /home/pascal/.local/share/virtualenvs/epistemically-uncertain-targets-H8pIDgDw/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int32 to dtype=bool with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.\n",
      "  warnings.warn(\n",
      "/home/pascal/.local/share/virtualenvs/epistemically-uncertain-targets-H8pIDgDw/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int32 to dtype=bool with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.\n",
      "  warnings.warn(\n",
      " 21%|██        | 4/19 [10:45<52:02, 208.15s/it, baseline/Breakout-MinAtar]/home/pascal/.local/share/virtualenvs/epistemically-uncertain-targets-H8pIDgDw/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int32 to dtype=bool with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.\n",
      "  warnings.warn(\n",
      "/home/pascal/.local/share/virtualenvs/epistemically-uncertain-targets-H8pIDgDw/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=bool with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.\n",
      "  warnings.warn(\n",
      "/home/pascal/.local/share/virtualenvs/epistemically-uncertain-targets-H8pIDgDw/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int32 to dtype=bool with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.\n",
      "  warnings.warn(\n",
      "/home/pascal/.local/share/virtualenvs/epistemically-uncertain-targets-H8pIDgDw/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=bool with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.\n",
      "  warnings.warn(\n",
      " 21%|██        | 4/19 [11:24<52:02, 208.15s/it, baseline_high_eps/Breakout-MinAtar]/home/pascal/.local/share/virtualenvs/epistemically-uncertain-targets-H8pIDgDw/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int32 to dtype=bool with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.\n",
      "  warnings.warn(\n",
      "/home/pascal/.local/share/virtualenvs/epistemically-uncertain-targets-H8pIDgDw/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=bool with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.\n",
      "  warnings.warn(\n",
      "/home/pascal/.local/share/virtualenvs/epistemically-uncertain-targets-H8pIDgDw/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int32 to dtype=bool with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.\n",
      "  warnings.warn(\n",
      "/home/pascal/.local/share/virtualenvs/epistemically-uncertain-targets-H8pIDgDw/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=bool with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.\n",
      "  warnings.warn(\n",
      " 21%|██        | 4/19 [12:03<52:02, 208.15s/it, baseline_low_eps/Breakout-MinAtar] /home/pascal/.local/share/virtualenvs/epistemically-uncertain-targets-H8pIDgDw/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int32 to dtype=bool with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.\n",
      "  warnings.warn(\n",
      "/home/pascal/.local/share/virtualenvs/epistemically-uncertain-targets-H8pIDgDw/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=bool with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.\n",
      "  warnings.warn(\n",
      "/home/pascal/.local/share/virtualenvs/epistemically-uncertain-targets-H8pIDgDw/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int32 to dtype=bool with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.\n",
      "  warnings.warn(\n",
      "/home/pascal/.local/share/virtualenvs/epistemically-uncertain-targets-H8pIDgDw/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=bool with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.\n",
      "  warnings.warn(\n",
      " 21%|██        | 4/19 [12:42<52:02, 208.15s/it, ens-ecppo/Breakout-MinAtar]       /home/pascal/.local/share/virtualenvs/epistemically-uncertain-targets-H8pIDgDw/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int32 to dtype=bool with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.\n",
      "  warnings.warn(\n",
      "/home/pascal/.local/share/virtualenvs/epistemically-uncertain-targets-H8pIDgDw/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=bool with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.\n",
      "  warnings.warn(\n",
      "/home/pascal/.local/share/virtualenvs/epistemically-uncertain-targets-H8pIDgDw/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int32 to dtype=bool with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.\n",
      "  warnings.warn(\n",
      "/home/pascal/.local/share/virtualenvs/epistemically-uncertain-targets-H8pIDgDw/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=bool with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.\n",
      "  warnings.warn(\n",
      " 26%|██▋       | 5/19 [14:09<48:13, 206.67s/it, baseline/Freeway-MinAtar]  /home/pascal/.local/share/virtualenvs/epistemically-uncertain-targets-H8pIDgDw/lib/python3.10/site-packages/gymnax/environments/minatar/freeway.py:283: UserWarning: Explicitly requested dtype <class 'jax.numpy.int64'> requested in array is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  return jnp.array(cars, dtype=jnp.int_)\n",
      "/home/pascal/.local/share/virtualenvs/epistemically-uncertain-targets-H8pIDgDw/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int32 to dtype=bool with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.\n",
      "  warnings.warn(\n",
      "/home/pascal/.local/share/virtualenvs/epistemically-uncertain-targets-H8pIDgDw/lib/python3.10/site-packages/gymnax/environments/minatar/freeway.py:283: UserWarning: Explicitly requested dtype <class 'jax.numpy.int64'> requested in array is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  return jnp.array(cars, dtype=jnp.int_)\n",
      "/home/pascal/.local/share/virtualenvs/epistemically-uncertain-targets-H8pIDgDw/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int32 to dtype=bool with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.\n",
      "  warnings.warn(\n",
      " 26%|██▋       | 5/19 [15:35<48:13, 206.67s/it, baseline_high_eps/Freeway-MinAtar]/home/pascal/.local/share/virtualenvs/epistemically-uncertain-targets-H8pIDgDw/lib/python3.10/site-packages/gymnax/environments/minatar/freeway.py:283: UserWarning: Explicitly requested dtype <class 'jax.numpy.int64'> requested in array is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  return jnp.array(cars, dtype=jnp.int_)\n",
      "/home/pascal/.local/share/virtualenvs/epistemically-uncertain-targets-H8pIDgDw/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int32 to dtype=bool with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.\n",
      "  warnings.warn(\n",
      "/home/pascal/.local/share/virtualenvs/epistemically-uncertain-targets-H8pIDgDw/lib/python3.10/site-packages/gymnax/environments/minatar/freeway.py:283: UserWarning: Explicitly requested dtype <class 'jax.numpy.int64'> requested in array is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  return jnp.array(cars, dtype=jnp.int_)\n",
      "/home/pascal/.local/share/virtualenvs/epistemically-uncertain-targets-H8pIDgDw/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int32 to dtype=bool with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.\n",
      "  warnings.warn(\n",
      " 26%|██▋       | 5/19 [17:00<48:13, 206.67s/it, baseline_low_eps/Freeway-MinAtar] /home/pascal/.local/share/virtualenvs/epistemically-uncertain-targets-H8pIDgDw/lib/python3.10/site-packages/gymnax/environments/minatar/freeway.py:283: UserWarning: Explicitly requested dtype <class 'jax.numpy.int64'> requested in array is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  return jnp.array(cars, dtype=jnp.int_)\n",
      "/home/pascal/.local/share/virtualenvs/epistemically-uncertain-targets-H8pIDgDw/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int32 to dtype=bool with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.\n",
      "  warnings.warn(\n",
      "/home/pascal/.local/share/virtualenvs/epistemically-uncertain-targets-H8pIDgDw/lib/python3.10/site-packages/gymnax/environments/minatar/freeway.py:283: UserWarning: Explicitly requested dtype <class 'jax.numpy.int64'> requested in array is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  return jnp.array(cars, dtype=jnp.int_)\n",
      "/home/pascal/.local/share/virtualenvs/epistemically-uncertain-targets-H8pIDgDw/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int32 to dtype=bool with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.\n",
      "  warnings.warn(\n",
      " 26%|██▋       | 5/19 [18:25<48:13, 206.67s/it, ens-ecppo/Freeway-MinAtar]       /home/pascal/.local/share/virtualenvs/epistemically-uncertain-targets-H8pIDgDw/lib/python3.10/site-packages/gymnax/environments/minatar/freeway.py:283: UserWarning: Explicitly requested dtype <class 'jax.numpy.int64'> requested in array is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  return jnp.array(cars, dtype=jnp.int_)\n",
      "/home/pascal/.local/share/virtualenvs/epistemically-uncertain-targets-H8pIDgDw/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int32 to dtype=bool with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.\n",
      "  warnings.warn(\n",
      "/home/pascal/.local/share/virtualenvs/epistemically-uncertain-targets-H8pIDgDw/lib/python3.10/site-packages/gymnax/environments/minatar/freeway.py:283: UserWarning: Explicitly requested dtype <class 'jax.numpy.int64'> requested in array is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  return jnp.array(cars, dtype=jnp.int_)\n",
      "/home/pascal/.local/share/virtualenvs/epistemically-uncertain-targets-H8pIDgDw/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int32 to dtype=bool with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.\n",
      "  warnings.warn(\n",
      " 32%|███▏      | 6/19 [21:08<1:00:24, 278.80s/it, baseline/SpaceInvaders-MinAtar]/home/pascal/.local/share/virtualenvs/epistemically-uncertain-targets-H8pIDgDw/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int32 to dtype=bool with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.\n",
      "  warnings.warn(\n",
      "/home/pascal/.local/share/virtualenvs/epistemically-uncertain-targets-H8pIDgDw/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=bool with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.\n",
      "  warnings.warn(\n",
      "/home/pascal/.local/share/virtualenvs/epistemically-uncertain-targets-H8pIDgDw/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int32 to dtype=bool with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.\n",
      "  warnings.warn(\n",
      "/home/pascal/.local/share/virtualenvs/epistemically-uncertain-targets-H8pIDgDw/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=bool with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.\n",
      "  warnings.warn(\n",
      "/home/pascal/.local/share/virtualenvs/epistemically-uncertain-targets-H8pIDgDw/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int32 to dtype=bool with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.\n",
      "  warnings.warn(\n",
      "/home/pascal/.local/share/virtualenvs/epistemically-uncertain-targets-H8pIDgDw/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=bool with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.\n",
      "  warnings.warn(\n",
      " 32%|███▏      | 6/19 [21:56<1:00:24, 278.80s/it, baseline_high_eps/SpaceInvaders-MinAtar]/home/pascal/.local/share/virtualenvs/epistemically-uncertain-targets-H8pIDgDw/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int32 to dtype=bool with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.\n",
      "  warnings.warn(\n",
      "/home/pascal/.local/share/virtualenvs/epistemically-uncertain-targets-H8pIDgDw/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=bool with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.\n",
      "  warnings.warn(\n",
      "/home/pascal/.local/share/virtualenvs/epistemically-uncertain-targets-H8pIDgDw/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int32 to dtype=bool with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.\n",
      "  warnings.warn(\n",
      "/home/pascal/.local/share/virtualenvs/epistemically-uncertain-targets-H8pIDgDw/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=bool with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.\n",
      "  warnings.warn(\n",
      "/home/pascal/.local/share/virtualenvs/epistemically-uncertain-targets-H8pIDgDw/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int32 to dtype=bool with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.\n",
      "  warnings.warn(\n",
      "/home/pascal/.local/share/virtualenvs/epistemically-uncertain-targets-H8pIDgDw/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=bool with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.\n",
      "  warnings.warn(\n",
      " 32%|███▏      | 6/19 [22:45<1:00:24, 278.80s/it, baseline_low_eps/SpaceInvaders-MinAtar] /home/pascal/.local/share/virtualenvs/epistemically-uncertain-targets-H8pIDgDw/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int32 to dtype=bool with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.\n",
      "  warnings.warn(\n",
      "/home/pascal/.local/share/virtualenvs/epistemically-uncertain-targets-H8pIDgDw/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=bool with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.\n",
      "  warnings.warn(\n",
      "/home/pascal/.local/share/virtualenvs/epistemically-uncertain-targets-H8pIDgDw/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int32 to dtype=bool with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.\n",
      "  warnings.warn(\n",
      "/home/pascal/.local/share/virtualenvs/epistemically-uncertain-targets-H8pIDgDw/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=bool with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.\n",
      "  warnings.warn(\n",
      "/home/pascal/.local/share/virtualenvs/epistemically-uncertain-targets-H8pIDgDw/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int32 to dtype=bool with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.\n",
      "  warnings.warn(\n",
      "/home/pascal/.local/share/virtualenvs/epistemically-uncertain-targets-H8pIDgDw/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=bool with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.\n",
      "  warnings.warn(\n",
      " 32%|███▏      | 6/19 [23:33<1:00:24, 278.80s/it, ens-ecppo/SpaceInvaders-MinAtar]       /home/pascal/.local/share/virtualenvs/epistemically-uncertain-targets-H8pIDgDw/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int32 to dtype=bool with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.\n",
      "  warnings.warn(\n",
      "/home/pascal/.local/share/virtualenvs/epistemically-uncertain-targets-H8pIDgDw/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=bool with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.\n",
      "  warnings.warn(\n",
      "/home/pascal/.local/share/virtualenvs/epistemically-uncertain-targets-H8pIDgDw/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int32 to dtype=bool with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.\n",
      "  warnings.warn(\n",
      "/home/pascal/.local/share/virtualenvs/epistemically-uncertain-targets-H8pIDgDw/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=bool with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.\n",
      "  warnings.warn(\n",
      "/home/pascal/.local/share/virtualenvs/epistemically-uncertain-targets-H8pIDgDw/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int32 to dtype=bool with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.\n",
      "  warnings.warn(\n",
      "/home/pascal/.local/share/virtualenvs/epistemically-uncertain-targets-H8pIDgDw/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=bool with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.\n",
      "  warnings.warn(\n",
      " 74%|███████▎  | 14/19 [37:45<11:28, 137.75s/it, baseline/FourRooms-misc]                  /home/pascal/.local/share/virtualenvs/epistemically-uncertain-targets-H8pIDgDw/lib/python3.10/site-packages/jax/_src/numpy/array_methods.py:66: UserWarning: Explicitly requested dtype <class 'jax.numpy.int64'> requested in astype is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  return lax_numpy.astype(arr, dtype)\n",
      " 74%|███████▎  | 14/19 [37:52<11:28, 137.75s/it, baseline_high_eps/FourRooms-misc]/home/pascal/.local/share/virtualenvs/epistemically-uncertain-targets-H8pIDgDw/lib/python3.10/site-packages/jax/_src/numpy/array_methods.py:66: UserWarning: Explicitly requested dtype <class 'jax.numpy.int64'> requested in astype is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  return lax_numpy.astype(arr, dtype)\n",
      " 74%|███████▎  | 14/19 [37:58<11:28, 137.75s/it, baseline_low_eps/FourRooms-misc] /home/pascal/.local/share/virtualenvs/epistemically-uncertain-targets-H8pIDgDw/lib/python3.10/site-packages/jax/_src/numpy/array_methods.py:66: UserWarning: Explicitly requested dtype <class 'jax.numpy.int64'> requested in astype is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  return lax_numpy.astype(arr, dtype)\n",
      " 74%|███████▎  | 14/19 [38:04<11:28, 137.75s/it, ens-ecppo/FourRooms-misc]       /home/pascal/.local/share/virtualenvs/epistemically-uncertain-targets-H8pIDgDw/lib/python3.10/site-packages/jax/_src/numpy/array_methods.py:66: UserWarning: Explicitly requested dtype <class 'jax.numpy.int64'> requested in astype is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  return lax_numpy.astype(arr, dtype)\n",
      " 95%|█████████▍| 18/19 [42:31<01:24, 84.59s/it, baseline/Pong-misc]                     /home/pascal/.local/share/virtualenvs/epistemically-uncertain-targets-H8pIDgDw/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.\n",
      "  warnings.warn(\n",
      " 95%|█████████▍| 18/19 [43:42<01:24, 84.59s/it, baseline_high_eps/Pong-misc]/home/pascal/.local/share/virtualenvs/epistemically-uncertain-targets-H8pIDgDw/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.\n",
      "  warnings.warn(\n",
      " 95%|█████████▍| 18/19 [44:53<01:24, 84.59s/it, baseline_low_eps/Pong-misc] /home/pascal/.local/share/virtualenvs/epistemically-uncertain-targets-H8pIDgDw/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.\n",
      "  warnings.warn(\n",
      " 95%|█████████▍| 18/19 [46:03<01:24, 84.59s/it, ens-ecppo/Pong-misc]       /home/pascal/.local/share/virtualenvs/epistemically-uncertain-targets-H8pIDgDw/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.\n",
      "  warnings.warn(\n",
      "100%|██████████| 19/19 [49:07<00:00, 155.11s/it, ens-ecppo/Pong-misc]\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "\n",
    "pbar = tqdm.tqdm(envs)\n",
    "for env in pbar:\n",
    "    if env in short_axis:\n",
    "        total_timesteps = int(1e7 * short_axis[env] / base)\n",
    "    else:\n",
    "        total_timesteps = 1e7\n",
    "    for name in configs.keys():\n",
    "        experiment_name = name + \"/\" + env\n",
    "    \n",
    "        if (experiment_name not in results.keys()):\n",
    "\n",
    "            pbar.set_postfix_str(experiment_name)\n",
    "            \n",
    "            config = {key: value for key, value in configs[name].items()} # copy the config\n",
    "            config[\"ENV_NAME\"] = env\n",
    "            config[\"TOTAL_TIMESTEPS\"] = total_timesteps\n",
    "\n",
    "            rng = jax.random.PRNGKey(42) # Each agent and environment uses the same random key\n",
    "            train_jit = jax.jit(make_train(config))\n",
    "            out = jax.vmap(train_jit)(jax.random.split(rng, 20)) # The random key is split into 20 keys for 20 runs per agent\n",
    "            results[experiment_name] = process_out(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results('ens-eccpo-results.pkl', drop_runner_state(results), configs=configs, force=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laplace-ECPPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "import numpy as np\n",
    "import optax\n",
    "from flax.linen.initializers import constant, orthogonal, variance_scaling, normal\n",
    "from typing import Sequence, NamedTuple, Any\n",
    "from functools import partial\n",
    "from flax.training.train_state import TrainState\n",
    "import distrax\n",
    "import gymnax\n",
    "from gymnax.wrappers.purerl import LogWrapper, FlattenObservationWrapper\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import pickle\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "from flax import core, struct\n",
    "from operator import mul\n",
    "from functools import reduce, partial\n",
    "from typing import Tuple\n",
    "\n",
    "# warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "# warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "\n",
    "def prod(tuple_):\n",
    "    if tuple_ == ():\n",
    "        return 0\n",
    "    return reduce(mul, tuple_, 1.0)\n",
    "\n",
    "\n",
    "def random_split_like_tree(rng_key, target=None, treedef=None):\n",
    "    if treedef is None:\n",
    "        treedef = jax.tree_util.tree_structure(target)\n",
    "    keys = jax.random.split(rng_key, treedef.num_leaves)\n",
    "    return jax.tree_util.tree_unflatten(treedef, keys)\n",
    "\n",
    "\n",
    "def tree_random_normal_like(rng_key, target):\n",
    "    keys_tree = random_split_like_tree(rng_key, target)\n",
    "    return jax.tree_util.tree_map(\n",
    "        lambda l, k: jax.random.normal(k, l.shape, l.dtype),\n",
    "        target,\n",
    "        keys_tree,\n",
    "    )\n",
    "\n",
    "def variance_scaling(w, base_scale):\n",
    "    variance = base_scale**2\n",
    "    fan_in = prod(w.shape[:-1])\n",
    "    variance /= max(1.0, fan_in)\n",
    "    scale = jnp.sqrt(variance) * jnp.ones(w.shape)\n",
    "    return scale\n",
    "\n",
    "class LaplaceState(NamedTuple):\n",
    "    fisher: core.FrozenDict[str, Any]\n",
    "    count: jnp.float32\n",
    "\n",
    "def laplace_mapping(params, noise, laplace_state: LaplaceState):\n",
    "    m = laplace_state.count\n",
    "    normalized_fisher = jax.tree_util.tree_map(lambda fi: fi / m,laplace_state.fisher)\n",
    "    output = jax.tree_util.tree_map(lambda x, z, lp: x + z / jnp.sqrt(lp), params, noise, normalized_fisher)\n",
    "    return output\n",
    "\n",
    "prior_bias_init = normal(0.1)\n",
    "\n",
    "class ActorEpistemicCritic(nn.Module):\n",
    "    action_dim: Sequence[int]\n",
    "    activation: str = \"tanh\"\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        if self.activation == \"relu\":\n",
    "            activation = nn.relu\n",
    "        elif self.activation == \"leaky_relu\":\n",
    "            activation = nn.leaky_relu\n",
    "        else:\n",
    "            activation = nn.tanh\n",
    "        actor_mean = nn.Dense(\n",
    "            64, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0)\n",
    "        )(x)\n",
    "        actor_mean = activation(actor_mean)\n",
    "        actor_mean = nn.Dense(\n",
    "            64, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0)\n",
    "        )(actor_mean)\n",
    "        actor_mean = activation(actor_mean)\n",
    "        actor_mean = nn.Dense(\n",
    "            self.action_dim, kernel_init=orthogonal(0.01), bias_init=constant(0.0)\n",
    "        )(actor_mean)\n",
    "        pi = distrax.Categorical(logits=actor_mean)\n",
    "\n",
    "        critic = nn.Dense(\n",
    "            64, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0)\n",
    "        )(x)\n",
    "        critic = activation(critic)\n",
    "        critic = nn.Dense(\n",
    "            64, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0)\n",
    "        )(critic)\n",
    "        critic = activation(critic)\n",
    "        critic = nn.Dense(1, kernel_init=orthogonal(1.0), bias_init=constant(0.0))(\n",
    "            critic\n",
    "        )\n",
    "\n",
    "        return pi, jnp.squeeze(critic, axis=-1)\n",
    "\n",
    "\n",
    "class Transition(NamedTuple):\n",
    "    done: jnp.ndarray\n",
    "    action: jnp.ndarray\n",
    "    mean_value: jnp.ndarray\n",
    "    ensemble_value: jnp.ndarray\n",
    "    reward: jnp.ndarray\n",
    "    log_prob: jnp.ndarray\n",
    "    obs: jnp.ndarray\n",
    "    info: jnp.ndarray\n",
    "\n",
    "def make_train(config):\n",
    "    config[\"NUM_UPDATES\"] = (\n",
    "        config[\"TOTAL_TIMESTEPS\"] // config[\"NUM_STEPS\"] // config[\"NUM_ENVS\"]\n",
    "    )\n",
    "    config[\"MINIBATCH_SIZE\"] = (\n",
    "        config[\"NUM_ENVS\"] * config[\"NUM_STEPS\"] // config[\"NUM_MINIBATCHES\"]\n",
    "    )\n",
    "\n",
    "    env_kwargs = {}\n",
    "    if config[\"ENV_NAME\"] == \"DeepSea-bsuite\":\n",
    "        env_kwargs[\"size\"] = 15\n",
    "    env, env_params = gymnax.make(config[\"ENV_NAME\"], **env_kwargs)\n",
    "    env = FlattenObservationWrapper(env)\n",
    "    env = LogWrapper(env)\n",
    "\n",
    "    def linear_schedule(count):\n",
    "        frac = 1.0 - (count // (config[\"NUM_MINIBATCHES\"] * config[\"UPDATE_EPOCHS\"])) / config[\"NUM_UPDATES\"]\n",
    "        return config[\"LR\"] * frac\n",
    "\n",
    "    def train(rng):\n",
    "\n",
    "        # INIT NETWORK\n",
    "        network = ActorEpistemicCritic(env.action_space(env_params).n, activation=config[\"ACTIVATION\"])\n",
    "        rng, _rng, ensemble_rng = jax.random.split(rng, 3)\n",
    "        init_x = jnp.zeros(env.observation_space(env_params).shape)\n",
    "        network_params = network.init(_rng, init_x)\n",
    "        ensemble_noise = jax.vmap(tree_random_normal_like, in_axes=(0, None))(jax.random.split(ensemble_rng, config[\"ENSEMBLE_SIZE\"]), network_params)\n",
    "\n",
    "        if config[\"ANNEAL_LR\"]:\n",
    "            tx = optax.chain(\n",
    "                optax.clip_by_global_norm(config[\"MAX_GRAD_NORM\"]),\n",
    "                optax.adam(learning_rate=linear_schedule, eps=1e-5),\n",
    "            )\n",
    "        else:\n",
    "            tx = optax.chain(optax.clip_by_global_norm(config[\"MAX_GRAD_NORM\"]), optax.adam(config[\"LR\"], eps=1e-5))\n",
    "        train_state = TrainState.create(\n",
    "            apply_fn=network.apply,\n",
    "            params=network_params,\n",
    "            tx=tx,\n",
    "        )\n",
    "\n",
    "        prior_scale = jax.tree_util.tree_map(partial(variance_scaling, base_scale=config[\"PRIOR_SCALE\"]), network_params)\n",
    "        init_laplace = jax.tree_util.tree_map(lambda x: 1 / x**2, prior_scale)\n",
    "        laplace_state = LaplaceState(init_laplace, 1.)\n",
    "\n",
    "        if \"UNCERTAINTY_SHIFT\" not in config.keys():\n",
    "            config[\"UNCERTAINTY_SHIFT\"] = 0.3\n",
    "        \n",
    "        if \"UNCERTAINTY_MULTIPLIER\" not in config.keys():\n",
    "            config[\"UNCERTAINTY_MULTIPLIER\"] = 15\n",
    "\n",
    "        if \"UNCERTAINTY_BASE\" not in config.keys():\n",
    "            config[\"UNCERTAINTY_BASE\"] = 0.5\n",
    "\n",
    "        if \"UNCERTAINTY_SCALE\" not in config.keys():\n",
    "            config[\"UNCERTAINTY_SCALE\"] = 1.5\n",
    "\n",
    "        if \"ADAPTIVE_SIGMA\" not in config.keys():\n",
    "            config[\"ADAPTIVE_SIGMA\"] = True\n",
    "\n",
    "        # INIT ENV\n",
    "        rng, _rng = jax.random.split(rng)\n",
    "        reset_rng = jax.random.split(_rng, config[\"NUM_ENVS\"])\n",
    "        obsv, env_state = jax.vmap(env.reset, in_axes=(0, None))(reset_rng,  env_params)\n",
    "\n",
    "        # TRAIN LOOP\n",
    "        def _update_step(runner_state, unused):\n",
    "            # COLLECT TRAJECTORIES\n",
    "            def _env_step(runner_state, unused):\n",
    "                train_state, env_state, laplace_state, last_obs, rng = runner_state\n",
    "\n",
    "                # CREATE THE ENSEMBLE:\n",
    "                ensemble = jax.vmap(laplace_mapping, in_axes=(None, 0, None))(train_state.params, ensemble_noise, laplace_state)\n",
    "                # SELECT ACTION\n",
    "                rng, _rng = jax.random.split(rng)\n",
    "                pi, mean_value = network.apply(train_state.params, last_obs)\n",
    "                _, ensemble_value = jax.vmap(network.apply, in_axes=(0, None), out_axes=-1)(ensemble, last_obs)\n",
    "                action = pi.sample(seed=_rng)\n",
    "                log_prob = pi.log_prob(action)\n",
    "\n",
    "                # STEP ENV\n",
    "                rng, _rng = jax.random.split(rng)\n",
    "                rng_step = jax.random.split(_rng, config[\"NUM_ENVS\"])\n",
    "                obsv, env_state, reward, done, info = jax.vmap(env.step, in_axes=(0,0,0,None))(\n",
    "                    rng_step, env_state, action, env_params\n",
    "                )\n",
    "                transition = Transition(\n",
    "                    done, action, mean_value, ensemble_value, reward, log_prob, last_obs, info\n",
    "                )\n",
    "                runner_state = (train_state, env_state, laplace_state, obsv, rng)\n",
    "                return runner_state, transition\n",
    "\n",
    "            runner_state, traj_batch = jax.lax.scan(\n",
    "                _env_step, runner_state, None, config[\"NUM_STEPS\"]\n",
    "            )\n",
    "\n",
    "            # CALCULATE ADVANTAGE\n",
    "            train_state, env_state, laplace_state, last_obs, rng = runner_state\n",
    "\n",
    "            ensemble = jax.vmap(laplace_mapping, in_axes=(None, 0, None))(train_state.params, ensemble_noise, laplace_state)\n",
    "            _, last_mean_val = network.apply(train_state.params, last_obs)\n",
    "            _, last_ens_val = jax.vmap(network.apply, in_axes=(0, None), out_axes=-1)(ensemble, last_obs)\n",
    "\n",
    "            def _calculate_gae(traj_batch, last_mean_val, last_ens_val):\n",
    "                def _get_advantages(gaes_and_next_values, transition):\n",
    "                    mean_gae, ens_gae, next_mean_value, next_ens_value = gaes_and_next_values\n",
    "                    done, mean_value, ens_value, reward = (\n",
    "                        transition.done,\n",
    "                        transition.mean_value,\n",
    "                        transition.ensemble_value,\n",
    "                        transition.reward,\n",
    "                    )\n",
    "                    # print(next_value.shape, value.shape)\n",
    "                    mean_delta = reward + config[\"GAMMA\"] * next_mean_value * (1 - done) - mean_value\n",
    "                    mean_gae = (\n",
    "                        mean_delta\n",
    "                        + config[\"GAMMA\"] * config[\"GAE_LAMBDA\"] * (1 - done) * mean_gae\n",
    "                    )\n",
    "\n",
    "                    ens_delta = reward[..., None] + config[\"GAMMA\"] * next_ens_value * (1 - done[..., None]) - ens_value\n",
    "                    ens_gae = (\n",
    "                        ens_delta\n",
    "                        + config[\"GAMMA\"] * config[\"GAE_LAMBDA\"] * (1 - done[..., None]) * ens_gae\n",
    "                    )\n",
    "\n",
    "                    return (mean_gae, ens_gae, mean_value, ens_value), (mean_gae, ens_gae)\n",
    "\n",
    "                _, (mean_advantages, ens_advantages) = jax.lax.scan(\n",
    "                    _get_advantages,\n",
    "                    (jnp.zeros_like(last_mean_val), jnp.zeros_like(last_ens_val), last_mean_val, last_ens_val),\n",
    "                    traj_batch,\n",
    "                    reverse=True,\n",
    "                    unroll=16,\n",
    "                )\n",
    "                return (mean_advantages, ens_advantages), (mean_advantages + traj_batch.mean_value, ens_advantages + traj_batch.ensemble_value)\n",
    "            # print(last_val.shape)\n",
    "            advantages, targets = _calculate_gae(traj_batch, last_mean_val, last_ens_val)\n",
    "\n",
    "            # UPDATE NETWORK\n",
    "            def _update_epoch(update_state, unused):\n",
    "                def _update_minbatch(carry: Tuple[TrainState, LaplaceState], batch_info):\n",
    "\n",
    "                    train_state, laplace_state = carry\n",
    "                    traj_batch, advantages, targets = batch_info\n",
    "                    n_transistions_seen = train_state.step * config[\"MINIBATCH_SIZE\"]\n",
    "                    \n",
    "                    def update_laplace(params, laplace_state, traj_batch):\n",
    "\n",
    "                        def grad_norm_estimate(params, traj_batch):\n",
    "                            value_net = lambda params, obs: network.apply(params, obs)[1].squeeze()\n",
    "\n",
    "                            obs_shape = traj_batch.obs.shape[1:]\n",
    "\n",
    "                            grads = jax.vmap(jax.grad(value_net), in_axes=(None, 0))(params, traj_batch.obs.reshape(-1, 1, *obs_shape))\n",
    "\n",
    "                            mean_output = jnp.mean(value_net(params, traj_batch.obs)**2)\n",
    "                            if config[\"ADAPTIVE_SIGMA\"]:\n",
    "                                sigma = config[\"TARGET_SIGMA\"] * mean_output\n",
    "                            else:\n",
    "                                sigma = config[\"TARGET_SIGMA\"]\n",
    "\n",
    "                            return jax.tree_util.tree_map(\n",
    "                                lambda l, g: 1 / l**2 + n_transistions_seen * jnp.mean(g * g / sigma**2, axis=0), \n",
    "                                prior_scale, grads\n",
    "                            )\n",
    "\n",
    "                        fisher, m = laplace_state\n",
    "                        fisher = jax.tree_util.tree_map(lambda o, n: (1 - config[\"FISHER_LR\"]) * o + n, fisher, grad_norm_estimate(params, traj_batch))\n",
    "                        m = (1 - config[\"FISHER_LR\"]) * m + 1\n",
    "                        return LaplaceState(fisher, m)\n",
    "\n",
    "                    def _loss_fn(params, traj_batch, gaes, targets):\n",
    "                        # RERUN NETWORK\n",
    "                        pi, value = network.apply(params, traj_batch.obs)\n",
    "                        log_prob = pi.log_prob(traj_batch.action)\n",
    "\n",
    "                        m_gae, e_gae = gaes\n",
    "                        m_targets, e_targets = targets\n",
    "\n",
    "                        # CALCULATE VALUE LOSS\n",
    "                        mean_value = traj_batch.mean_value\n",
    "                        value_pred_clipped = mean_value + (\n",
    "                            value - mean_value\n",
    "                        ).clip(-config[\"CLIP_EPS\"], config[\"CLIP_EPS\"])\n",
    "\n",
    "        \n",
    "                        value_losses = jnp.square(value - m_targets)\n",
    "                        value_losses_clipped = jnp.square(value_pred_clipped - m_targets)\n",
    "                        value_loss = (\n",
    "                            0.5 * jnp.maximum(value_losses, value_losses_clipped).mean()\n",
    "                        )\n",
    "\n",
    "                        # CALCULATE ACTOR LOSS\n",
    "                        ratio = jnp.exp(log_prob - traj_batch.log_prob)\n",
    "\n",
    "                        m_gae = (m_gae - m_gae.mean()) / (m_gae.std() + 1e-8)\n",
    "                        e_gae = (e_gae - e_gae.mean()) / (e_gae.std() + 1e-8)\n",
    "\n",
    "                        gae_uncertainty = e_gae.std(axis=-1)\n",
    "\n",
    "                        \n",
    "                        if config[\"UNCERTAINTY\"]:\n",
    "                            squashed_uncertainty = config[\"UNCERTAINTY_BASE\"] + config[\"UNCERTAINTY_SCALE\"] * jax.nn.sigmoid( config[\"UNCERTAINTY_MULTIPLIER\"] * (-gae_uncertainty + config[\"UNCERTAINTY_SHIFT\"] ))\n",
    "                        else:\n",
    "                            squashed_uncertainty = jnp.array(1.)\n",
    "\n",
    "                        loss_actor1 = ratio * m_gae # average loss over all values\n",
    "                        loss_actor2 = (\n",
    "                            jnp.clip(\n",
    "                                ratio,\n",
    "                                1.0 - config[\"CLIP_EPS\"] * squashed_uncertainty,\n",
    "                                1.0 + config[\"CLIP_EPS\"] * squashed_uncertainty,\n",
    "                            )\n",
    "                            * m_gae\n",
    "                        )\n",
    "\n",
    "                        percentage_clipped = (jnp.abs(ratio - 1) > config[\"CLIP_EPS\"] * squashed_uncertainty).mean()\n",
    "\n",
    "                        loss_actor = -jnp.minimum(loss_actor1, loss_actor2)\n",
    "\n",
    "                        loss_argmin = (loss_actor1 > loss_actor2).mean()\n",
    "                        loss_actor = loss_actor.mean()\n",
    "                        entropy = pi.entropy().mean()\n",
    "\n",
    "                        total_loss = (\n",
    "                            loss_actor\n",
    "                            + config[\"VF_COEF\"] * value_loss\n",
    "                            - config[\"ENT_COEF\"] * entropy\n",
    "                        )\n",
    "                        return total_loss, dict(value_loss=value_loss, \n",
    "                                                loss_actor=loss_actor, \n",
    "                                                entropy=entropy, \n",
    "                                                mean_uncertainty=gae_uncertainty.mean(), \n",
    "                                                min_uncertainty=gae_uncertainty.min(), \n",
    "                                                max_uncertainty=gae_uncertainty.max(),\n",
    "                                                min_gae = m_gae.min(),\n",
    "                                                max_gae = m_gae.max(),\n",
    "                                                percentage_clipped=percentage_clipped,\n",
    "                                                loss_argmin=loss_argmin,\n",
    "                                                mean_squashed_uncertainty=squashed_uncertainty.mean(),\n",
    "                                                max_squashed_uncertainty=squashed_uncertainty.max(),\n",
    "                                                min_squashed_uncertainty=squashed_uncertainty.min())\n",
    "\n",
    "                    grad_fn = jax.value_and_grad(_loss_fn, has_aux=True)\n",
    "                    total_loss, grads = grad_fn(\n",
    "                        train_state.params, traj_batch, advantages, targets\n",
    "                    )\n",
    "                    train_state = train_state.apply_gradients(grads=grads)\n",
    "\n",
    "                    laplace_state = update_laplace(train_state.params, laplace_state, traj_batch)\n",
    "\n",
    "                    return (train_state, laplace_state), total_loss\n",
    "\n",
    "                train_state, traj_batch, advantages, laplace_state, targets, rng = update_state\n",
    "                rng, _rng = jax.random.split(rng)\n",
    "                batch_size = config[\"MINIBATCH_SIZE\"] * config[\"NUM_MINIBATCHES\"]\n",
    "                assert (\n",
    "                    batch_size == config[\"NUM_STEPS\"] * config[\"NUM_ENVS\"]\n",
    "                ), \"batch size must be equal to number of steps * number of envs\"\n",
    "                permutation = jax.random.permutation(_rng, batch_size)\n",
    "                batch = (traj_batch, advantages, targets)\n",
    "\n",
    "                batch = jax.tree_util.tree_map(\n",
    "                    lambda x: x.reshape((batch_size,) + x.shape[2:]), batch\n",
    "                )\n",
    "                shuffled_batch = jax.tree_util.tree_map(\n",
    "                    lambda x: jnp.take(x, permutation, axis=0), batch\n",
    "                )\n",
    "                minibatches = jax.tree_util.tree_map(\n",
    "                    lambda x: jnp.reshape(\n",
    "                        x, [config[\"NUM_MINIBATCHES\"], -1] + list(x.shape[1:])\n",
    "                    ),\n",
    "                    shuffled_batch,\n",
    "                )\n",
    "                (train_state, laplace_state), total_loss = jax.lax.scan(\n",
    "                    _update_minbatch, (train_state, laplace_state), minibatches\n",
    "                )\n",
    "                update_state = (train_state, traj_batch, advantages, laplace_state, targets, rng)\n",
    "                return update_state, total_loss\n",
    "\n",
    "            update_state = (train_state, traj_batch, advantages, laplace_state, targets, rng)\n",
    "            update_state, loss_info = jax.lax.scan(\n",
    "                _update_epoch, update_state, None, config[\"UPDATE_EPOCHS\"]\n",
    "            )\n",
    "            train_state = update_state[0]\n",
    "            laplace_state = update_state[3]\n",
    "            metric = traj_batch.info\n",
    "            rng = update_state[-1]\n",
    "\n",
    "            runner_state = (train_state, env_state, laplace_state, last_obs, rng)\n",
    "            info = dict(\n",
    "                metrics=metric,\n",
    "                loss_info=loss_info\n",
    "            )\n",
    "            return runner_state, info\n",
    "        \n",
    "        if config[\"SAVE_STATE_FREQ\"]:\n",
    "            outer_updates = config[\"NUM_UPDATES\"] // config[\"SAVE_STATE_FREQ\"]\n",
    "            inner_updates = config[\"SAVE_STATE_FREQ\"]\n",
    "            left_over = config[\"NUM_UPDATES\"] - inner_updates * outer_updates \n",
    "            print(inner_updates, outer_updates, left_over)\n",
    "        else:\n",
    "            inner_updates = config[\"NUM_UPDATES\"]\n",
    "            outer_updates = 1\n",
    "            left_over = 0\n",
    "\n",
    "        # start with the left_over steps:\n",
    "        rng, _rng = jax.random.split(rng)\n",
    "        runner_state = (train_state, env_state, laplace_state, obsv, _rng)\n",
    "        runner_state, initial_info = jax.lax.scan(\n",
    "            _update_step, runner_state, None, left_over\n",
    "        )\n",
    "\n",
    "        def _outer_update_step(update_state, unused):\n",
    "            runner_state, info = jax.lax.scan(\n",
    "                _update_step, update_state, None, inner_updates\n",
    "            )\n",
    "            return runner_state, (runner_state, info)\n",
    "\n",
    "        # then do the remaining updates\n",
    "        runner_state, (states, info) = jax.lax.scan(\n",
    "            _outer_update_step, runner_state, None, outer_updates\n",
    "        )\n",
    "\n",
    "        info = jax.tree_util.tree_map(lambda x: x.reshape(-1, *x.shape[2:]), info)\n",
    "        info = jax.tree_util.tree_map(lambda x, y: jnp.concatenate([x, y], axis=0), initial_info, info)\n",
    "\n",
    "        info[\"runner_state\"] = states\n",
    "        info['laplace_state'] = runner_state[2]\n",
    "              \n",
    "        return info\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lp_ecppo_config = {\n",
    "    \"LR\": 5e-3,\n",
    "    \"NUM_ENVS\": 64,\n",
    "    \"NUM_STEPS\": 128,\n",
    "    \"TOTAL_TIMESTEPS\": 1e7,\n",
    "    \"UPDATE_EPOCHS\": 4,\n",
    "    \"NUM_MINIBATCHES\": 8,\n",
    "    \"GAMMA\": 0.99,\n",
    "    \"GAE_LAMBDA\": 0.95,\n",
    "    \"CLIP_EPS\": 0.2,\n",
    "    \"ENT_COEF\": 0.01,\n",
    "    \"VF_COEF\": .5,\n",
    "    \"MAX_GRAD_NORM\": 0.5,\n",
    "    \"ACTIVATION\": \"leaky_relu\",\n",
    "    \"ENV_NAME\": \"FourRooms-misc\",\n",
    "    \"ANNEAL_LR\": True,\n",
    "    \"ENSEMBLE_SIZE\": 5,\n",
    "    \"UNCERTAINTY\": True,\n",
    "    \"SAVE_STATE_FREQ\": False,\n",
    "    \"PRIOR_SCALE\": 2.0,\n",
    "    \"FISHER_LR\": 1e-2,\n",
    "    \"TARGET_SIGMA\": 0.1,\n",
    "    \"ADAPTIVE_SIGMA\": True,\n",
    "    \"UNCERTAINTY_SHIFT\": 0.3,\n",
    "    \"UNCERTAINTY_MULTIPLIER\": 15,\n",
    "}\n",
    "\n",
    "rng = jax.random.PRNGKey(42)\n",
    "train_jit = jax.jit(make_train(lp_ecppo_config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "configs = {\n",
    "    'laplace-ecppo': lp_ecppo_config,\n",
    "}\n",
    "\n",
    "load = False\n",
    "\n",
    "if load:\n",
    "    with open('results.pkl', 'rb') as f:\n",
    "        results, _ = pickle.load(f)\n",
    "else:\n",
    "    results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CartPole-v1', 'Acrobot-v1', 'MountainCar-v0', 'Asterix-MinAtar', 'Breakout-MinAtar', 'Freeway-MinAtar', 'SpaceInvaders-MinAtar', 'Catch-bsuite', 'DeepSea-bsuite', 'MemoryChain-bsuite', 'UmbrellaChain-bsuite', 'DiscountingChain-bsuite', 'MNISTBandit-bsuite', 'SimpleBandit-bsuite', 'FourRooms-misc', 'MetaMaze-misc', 'BernoulliBandit-misc', 'GaussianBandit-misc', 'Pong-misc']\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "envs = [env for env in gymnax.registered_envs if isinstance(gymnax.make(env)[0].action_space(), gymnax.environments.spaces.Discrete)]\n",
    "\n",
    "\n",
    "print(envs)\n",
    "\n",
    "base = 1e7 // (64 * 128)\n",
    "short_axis = {\n",
    "    \"CartPole-v1\": 200,\n",
    "    \"Pong-misc\": 400,\n",
    "    \"UmbrellaChain-bsuite\": 50,\n",
    "    \"FourRooms-misc\": 100,\n",
    "    \"DiscountingChain-bsuite\": 200,\n",
    "    \"GaussianBandit-misc\": 50,\n",
    "    \"Catch-bsuite\": 50,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All envs: ['CartPole-v1', 'Pendulum-v1', 'Acrobot-v1', 'MountainCar-v0', 'MountainCarContinuous-v0', 'Asterix-MinAtar', 'Breakout-MinAtar', 'Freeway-MinAtar', 'SpaceInvaders-MinAtar', 'Catch-bsuite', 'DeepSea-bsuite', 'MemoryChain-bsuite', 'UmbrellaChain-bsuite', 'DiscountingChain-bsuite', 'MNISTBandit-bsuite', 'SimpleBandit-bsuite', 'FourRooms-misc', 'MetaMaze-misc', 'PointRobot-misc', 'BernoulliBandit-misc', 'GaussianBandit-misc', 'Reacher-misc', 'Swimmer-misc', 'Pong-misc']\n",
      "Used envs: ['CartPole-v1', 'Acrobot-v1', 'MountainCar-v0', 'Asterix-MinAtar', 'Breakout-MinAtar', 'Freeway-MinAtar', 'SpaceInvaders-MinAtar', 'Catch-bsuite', 'DeepSea-bsuite', 'MemoryChain-bsuite', 'UmbrellaChain-bsuite', 'DiscountingChain-bsuite', 'MNISTBandit-bsuite', 'SimpleBandit-bsuite', 'FourRooms-misc', 'MetaMaze-misc', 'BernoulliBandit-misc', 'GaussianBandit-misc', 'Pong-misc']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [16:06<00:00, 50.86s/it, laplace-ecppo/Pong-misc]              \n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"All envs: {gymnax.registered_envs}\")\n",
    "print(f\"Used envs: {envs}\")\n",
    "\n",
    "n_envs = len(envs)\n",
    "\n",
    "force_agent = []\n",
    "force_envs = []\n",
    "\n",
    "pbar = tqdm.tqdm(envs)\n",
    "for env in pbar:\n",
    "    if env in short_axis:\n",
    "        total_timesteps = int(1e7 * short_axis[env] / base)\n",
    "    else:\n",
    "        total_timesteps = 1e7\n",
    "    for name in configs.keys():\n",
    "        experiment_name = name + \"/\" + env\n",
    "\n",
    "        if (experiment_name not in results.keys()) or (name in force_agent) and (env in force_envs):\n",
    "            pbar.set_postfix_str(experiment_name)\n",
    "            \n",
    "            config = {key: value for key, value in configs[name].items()} # copy the config\n",
    "            config[\"ENV_NAME\"] = env\n",
    "            config[\"TOTAL_TIMESTEPS\"] = total_timesteps\n",
    "\n",
    "            rng = jax.random.PRNGKey(42)\n",
    "            train_jit = jax.jit(make_train(config))\n",
    "            out = jax.vmap(train_jit)(jax.random.split(rng, 20))\n",
    "\n",
    "            results[experiment_name] = process_out(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results('laplace-ecppo-results.pkl', drop_runner_state(results), configs=configs, force=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "epistemically-uncertain-targets-H8pIDgDw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
